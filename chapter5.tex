\chapter{Experiments/Evaluation}
\label{chp:eval}
\label{sec:evaluation}
%Why we need evaluation\label{sec:relevance}
The main contribution of this thesis is to introduce a new approach for explanations(Hierarchical Intervention). We evaluate this new approach in a number of different ways. We perform qualitative evaluation and speed/scalability evaluation.

In order to compare the different clustering techniques that we use in hierarchical intervention, we did a couple of experiments. Our solution relies on the fact that the higher the influence or intensity are a measure of the value of our explanations. We used a number of observations on the NYC TLC data to see how each clustering technique fares against the others.

\textbf{Experimental Setup}. In order to compare the different spatial partitioning techniques, we set up an environment for experiments. Since the quality of the explanations does not depend on the speed and scale of the system, these experiments were performed on a single node machine with an i7 6400 3.5 GHz CPU and 8GB of RAM. The dataset used for this experiment was the NYC TLC data for January 2016. This data contains ~10 million trips.

We found that R-Tree and R* Tree partitioning is usually more stable than K Means or Greedy Clustering. The stability of the clustering technique is referring to its monotonicity of the explanation as a function of the number of clusters. Fig.~\ref{fig:hieint_passenger_count} shows the influence against the number of clusters where the observation is the average passenger count. The influence and intensity are very sinusoidal. Fig.~\ref{fig:clustering_comparison} shows another observation and a plot of the influence of R-Tree and R*-Tree as well. The influence as a function of the number of clusters is a lot more monotonic here. Furthermore, R* Tree has a higher influence on average compared to other approaches making it a lot more suitable for explanations.
%Reasons for selecting evaluation criteria
The qualitative evaluation that we define is based on K-Folds cross validation. The reason for selecting these evaluations is to measure how the top explanations provided by each approach fair as stand alone parts of data as well as their effects on the entire dataset as a whole. Other than qualitative analysis we look at the speed and scalability of each approach for performance evaluation.

\begin{figure}[H]
\includegraphics[width=\columnwidth]{clustering_comparison}
\caption{Comparison of Influence against number of clusters for K-Means, R-Tree and R*-Tree}
\label{fig:clustering_comparison}
\end{figure}

\begin{figure}[h]
\includegraphics[width=\columnwidth]{hieint_passenger_count}
\caption{Comparison of Influence and Intensity against level of hierarchy with average passenger count as observation}
\label{fig:hieint_passenger_count}
\end{figure}
%How does each approach compare
\textbf{Influence Comparison.}
\begin{figure}[h]
\includegraphics[width=\columnwidth]{Top_explanations_for_avg_tip_percentage_influence}
\caption{Influence for Spatial explanation for average tip percentage(Lower is better)}
\label{fig:influence_comparison}
\end{figure}

Fig.~\ref{fig:influence_comparison} shows the comparison between the influence of different approaches. Salient features and Aggravation have a very low influence compared to Intervention and hierarchical intervention. This is because the explanations provided by those approaches cover a very small portion of the entire dataset. This makes removing the associated data have a negligible impact on the entire data.

\section{K-Folds cross validation}
In order to qualitatively assess the solutions, we use K-Folds cross validation\citep{refaeilzadeh2009cross}. Since we do not have ground truth for our explanations, we have to make a compromise in terms of estimating how good our solution is. Our assumption here is that in general, a good explanation would stand out on unseen data i.e. given a good explanation on part of the data, it would give similar results on unseen parts of the data. This is the same assumption that was made by \cite{chirigati2016data} in their evaluation of the Data Polygamy framework. We split the data into $k$ parts. The data was divides by the month of the year. This is because there are specific observable patterns in each month, e.g. weekends have less density of trips than the weekdays for the dataset we evaluated. One of the parts is used to find the explanation(the training data). The rest of the data is the test data. We also calculate an explanation of the test data. The relative distance between the explanation index of the test data and the explanation index of the explanation provided by the training data when applied to the test data is used as a measure of evaluation.

The reason for using K-Folds evaluation in this way is because there is a lack of data with the ground truth values. We can see similar approaches for evaluation used in the works that our approach has been inspired from. The evaluation in the work by \cite{roy2014formal} is less relaxed because they only look at the magnitude of intervention for evaluation, whereas, \cite{chirigati2016data} also use the NYC TLC data for evaluation using the assumption that the data for 2011 and 2012 have the same pattern. In order to measure the quality of the explanations provided by hierarchical intervention we compare it with intervention and aggravation.

There is a reason for applying the explanation provided by the training data to the test data. The observation can be anything. The size of the training and test data is different depending on $k$. The observation can depend on the size of the data. This adds a bias when the explanation index for the training data is compared with the explanation index of the test data. Thus, we must apply the explanation provided by the training data on the test data and compare it with the explanation when we apply the same approach to the test data.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{kfolds.png}
\caption{The result for k-folds evaluation shows good results when the approach is used in favor of influence(lower is better) for observations represented by Queries~\ref{qry:o1},~\ref{qry:o2}, and ~\ref{qry:o3}}
\label{fig:kfolds}
\end{figure}

\textbf{Experimental Setup}. We performed the evaluation on the first 5 months of data from the NYC yellow cab dataset from 2016. The data spanned ~ 50 million trips. The $k$ for our evaluation was $5$. This value was chosen after a series of experiments. We wanted the size of each partition to be large enough to support our generalization assumption. If the solution space for evaluation is small, it leads to inconsistent results. Each month represented a partition for k-folds evaluation. According to our analysis, we found that the approach works very well with low values of alpha. Fig.~\ref{fig:kfolds} shows some of the results of our evaluation for a number of observations. The lower the value of the relative distance, the better the results.

The relative distance in the K Folds cross validation represents how much the explanation impact changes when we look at the explanations in the context of unseen data. A relative distance of zero represents that the explanation has the same impact on unseen data as if we had seen the data. A relative distance of one would represent that the top explanation for the unseen data has nothing in common with the explanation of seen data. Let $e_{test}$ be the explanation index for the test set. Let $e_{train}$ be the explanation index for the training set. Then,

$$relative\_distance = \frac{|e_{test}-e_{train}|}{|e_{test}+e_{train}|}$$

The relative distance metric was chosen because of its nature of showing a steep decline when two solutions do not match. It is also ideal because it caters to the different types of observations that a user can specify. Since the observations in our system are defined as an arithmetic operation over aggregate queries, they do not have bounds on their values. Thus, it makes relative distance a useful metric for measuring similarity. 

Here are the observations that we show in Fig.~\ref{fig:kfolds}:
\renewcommand{\lstlistingname}{Query}% Listing -> Algorithm
\begin{lstlisting}[language=SQL, caption=o1 for Fig.~\ref{fig:kfolds}, label=qry:o1]
Select count(*) as q1 from data where payment_type=1;
Select count(*) as q2 from data where payment_type=2;
Observation = q1/q2
\end{lstlisting}

\begin{lstlisting}[language=SQL, caption=o2 for Fig.~\ref{fig:kfolds}, label=qry:o2]
Select AVG(trip_distance) as q1 where passenger_count=4;
Observation = q1
\end{lstlisting}

\begin{lstlisting}[language=SQL, caption=o3 for Fig.~\ref{fig:kfolds}, label=qry:o3]
Select count(*) as q1 from data where vendorid=1;
Select count(*) as q2 from data where vendorid=2;
Observation = q1/q2
\end{lstlisting}

\section{Precision and Recall}
\begin{wrapfigure}{L}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{outbreakheatmap.png}
  \end{center}
  \caption{A heatmap showing sythesized data}
  \label{fig:synthesized_data}
\end{wrapfigure}
% \begin{figure}[h]
% \includegraphics[width=\columnwidth]{outbreakheatmap.png}
% \caption{A heatmap showing sythesized data}
% \label{fig:synthesized_data}
% \end{figure}

Since it is very difficult to find datasets with the ground truth we used synthetic datasets to evaluate our solution\citep{maciejewski2009generating}. The synthetic dataset is fabricated using real data from Indiana Public Health Emergency Surveillance System. This data contains information about Patients in Indiana and their health issues over time and location. We can calculate precision and recall using this data\citep{powers2011evaluation}. The data is synthesized in such a way that it shows similar patterns to the actual data. Synthetic data related to outbreaks of certain diseases is synthesized into this data. In order to evaluate our system, we used the temporal component of the outbreak and the type of disease as our observation. To be specific, the synthetic data consisted of an outbreak of Gastrointestinal infection in July. Our observation was the ratio of the number of gastrointestinal infection incidents between June and July when there's an infection against the number of gastrointestinal infection incidents between June and July in synthesized data where there wasn't an outbreak. Precision and Recall was calculated by comparing the areas of the top ten explanations related to the ground truth with the approximate area covered by the outbreak data. We cannot use the individual points to calculate precision and recall because there is a lot of data unrelated to the outbreak which occurs in proximity. To put things into context, each instance of the synthesized data contains ~2 million records. The data related to the outbreak consists of a few dozen records. Fig.~\ref{fig:synthesized_data} shows a heatmap of all the points in the synthesized data. The outbreak for this particular instance only affects a small part of Indiana to the North West just below Lake Michigan. Fig.~\ref{fig:precisionrecall} shows the points representing the outbreak as well as some of the top polygons returned using Hierarchical Intervention.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{precisionrecall.png}
\caption{Synthetic Outbreak data. The circles show the ground truth while the polygons show the relevant explanations produced by our system}
\label{fig:precisionrecall}
\end{figure}

Let $E$ be the area represented by our polygons. Let $O$ be the area represented by the Outbreak points. Then,
$$Precision = \frac{E \cap O}{E} = 0.31$$

For calculating recall, we can use the actual outbreak points since it doesn't require false positives.
$$Recall = \frac{true\_positives}{false\_negatives} = 0.74$$




\subsection{Comparison}
We compared the precision and recall of Hierarchical Intervention with Aggravation and Intervention.
\\
\textbf{Aggravation}. We used the same observation for aggravation that we did for Hierarchical Intervention. From the top ten explanations, only one of the explanation was able to predict one of the 27 correct tuples in the ground truth.
$$Precision = 0.01$$
$$Recall = 0.04$$


\textbf{Intervention}. Finally, Intervention was used to find an explanation. Since there was only one hospital in the area of the outbreak, Intervention was able to provide that hospital as a relevant predicate in an explanation. This resulted in all the ground truth points being included in the explanation! However, since there are a large number of tuples related to this predicate, it resulted in a low precision.

$$Precision = 0.0003,\ Recall = 1$$


% \begin{wrapfigure}{r}{0.5\textwidth}
%   \begin{center}
%     \includegraphics[width=0.48\textwidth]{precisionrecall.png}
%   \end{center}
%   \caption{Synthetic Outbreak data. The circles show the ground truth while the polygons show the relevant explanations produced by our system}
% \label{fig:spatial_spatial_vertical}
% \end{wrapfigure}



% \textbf{Influence and Intensity for Hierarchical Intervention}
% \begin{figure}[H]
% \includegraphics[width=\columnwidth]{heiint_tip_percentage}
% \caption{Comparison of Influence and Intensity against level of hierarchy with average tip percentage as observation}
% \label{fig:hieint_tip_percentage}
% \end{figure}




% \begin{figure}[H]
% \includegraphics[width=\columnwidth]{count_initial}
% \caption{The number of trips for each zone by pickup zone}
% \label{fig:count_initial}
% \end{figure}

% \begin{figure}[H]
% \includegraphics[width=\columnwidth]{top_level_2}
% \caption{Number of rows that represent the top explanation for average tip percentage at level 2}
% \label{fig:top_level_2}
% \end{figure}

% \begin{figure}[H]
% \includegraphics[width=\columnwidth]{top_level_3}
% \caption{Number of rows that represent the top explanation for average tip percentage at level 3}
% \label{fig:top_level_3}
% \end{figure}

% \begin{figure}[H]
% \includegraphics[width=\columnwidth]{level_3_removed}
% \caption{The cartogram for the data when we remove he level 3 explanation}
% \label{fig:level_3_removed}
% \end{figure}


\section{Speed and Scalability}
\label{sec:speed}
The speed of each approach is simply the time taken to calculate the explanations. The time varies depending on the size of the data. For performance evaluation, we used the NYC TLC data for 2016. The data is divided into months. As we increase the number of months to be processed, the time taken for each approach increases almost linearly for aggravation, intervention and hierarchical intervention. For salient features, the time increases slightly exponentially. The results of our evaluation are displayed in Fig.~\ref{fig:performance}

\textbf{Experimental Setup}. The speed of each approach is simply the time taken to calculate the explanations. Since all the approaches are implemented on top of distributed systems, the time varies depending on the number of nodes used. We experimented with calculating explanations for 1,3, and 5 months in a distributed environment. For the cluster, we used Google Compute Engine nodes(1 vCPU, 3.5GB RAM, 4 nodes). All the nodes in our cluster had the same specifications. The Aggravation, Intervention and Hierarchical Intervention were tested on Apache Spark running on YARN while the Salient Features were generated with the Data Polygamy Framework running on YARN.
\begin{figure}[h]
\includegraphics[width=\columnwidth]{performance}
\caption{Time taken by each approach as we increase the size of data}
\label{fig:performance}
\end{figure}

The run times that were observed can be explained as follows. Aggravation takes the least amount of time of the compared approaches. Intervention costs more than aggravation because it includes the additional step of calculating statistics for the attributes as well as iterating the table. Hierarchical Intervention takes more time than aggravation but less time than intervention. This is because of several factors. One of the factors is that when we are using intervention and aggravation, we are using several attributes, on the other hand, hierarchical intervention only uses the spatial attribute. On the other hand, we are creating a hierarchy. The in the case of this analysis we only used one query as the observation for the sake of comparison. However, multiple queries can affect the comparison as well. The time taken by Salient Features is also affected by a number of factors. First, the implementation of the salient features that we use is the one used in the Data Polygamy Framework \citep{chirigati2016data}. This approach uses the MapReduce framework. The other approaches use the Spark framework. Since Spark makes use of an execution plan which is more efficient than MapReduce, it is a factor in the slower runtime of the Salient Features approach. Thus, when comparing salient features to the other approaches, this can be considered something similar to black box testing. The most expensive computation step in salient features is the interpolation step which doesn't occur in the other approaches.


% \begin{figure}[H]
% \includegraphics[width=\columnwidth]{day_comparison}
% \caption{Number of trips against trip day}
% \label{fig:day_comparison}
% \end{figure}
